{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Querie Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "#get the reddit web scraper library\n",
    "folder_path = os.path.abspath(\"..\")\n",
    "\n",
    "if folder_path not in sys.path:\n",
    "    sys.path.append(folder_path)\n",
    "\n",
    "from database import KnowledgeBase\n",
    "import reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#get queries text example\n",
    "with open(\"../gradio-tests/queries_0a891.json\") as file:\n",
    "    out_json = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Web Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "from datetime import datetime\n",
    "\n",
    "#build database\n",
    "db = KnowledgeBase()\n",
    "#start session\n",
    "db.start_session()\n",
    "\n",
    "#save topic\n",
    "topic = \"javascript\"\n",
    "#get difficulty\n",
    "difficulty = \"beginner\"\n",
    "#get search query\n",
    "search_query = out_json[difficulty][\"query\"]\n",
    "#create search\n",
    "search_results = search(search_query, advanced=True, num_results=5)\n",
    "\n",
    "index = 0\n",
    "#search results\n",
    "results_data = []\n",
    "#loop through results\n",
    "for result in search_results:\n",
    "    index += 1\n",
    "\n",
    "    print(f\"Round {index}\")\n",
    "    if not db.find_url(result.url):\n",
    "        #track current result\n",
    "        curr_data = {}\n",
    "        #save resource url\n",
    "        curr_data[\"resource\"] = result.url\n",
    "        #save resource title\n",
    "        curr_data[\"title\"] = result.title\n",
    "        #save resource description\n",
    "        curr_data[\"description\"] = result.description.replace(\"'\", \"\")\n",
    "        #save resource topic\n",
    "        curr_data[\"topic\"] = topic\n",
    "        #save resource difficulty\n",
    "        curr_data[\"difficulty\"] = difficulty\n",
    "        #save resource validation\n",
    "        curr_data[\"validated\"] = False\n",
    "        #save resource found time\n",
    "        curr_data[\"found_time\"] = datetime.now()\n",
    "        #append data to list\n",
    "        results_data.append(curr_data)\n",
    "    else:\n",
    "        print(f\"Link: {result.url} already exists in database\")\n",
    "        continue\n",
    "\n",
    "    #check to see if we found our five links\n",
    "    if len(results_data) == 5:\n",
    "        break\n",
    "\n",
    "#check for missing resources\n",
    "if len(results_data) < 5:\n",
    "    #build search query for reddit\n",
    "    reddit_query = f\"Reddit {out_json[difficulty][\"query\"]}\"\n",
    "    #get reddit search results\n",
    "    reddit_results = search(reddit_query, advanced=True, num_results=5)\n",
    "\n",
    "    index = 0\n",
    "    #go through search results\n",
    "    for result in reddit_results:\n",
    "        \n",
    "        index += 1\n",
    "        print(f\"Reddit Round: {index}\")\n",
    "\n",
    "        if result.url.split(\".\")[1] == \"reddit\":\n",
    "            #get the scraped resources\n",
    "            scraped_resources = reddit.get_links(result.url)\n",
    "            #check to see if we get scraped resources\n",
    "            if scraped_resources:\n",
    "                #loop through scraped resources\n",
    "                for scraped_url in scraped_resources:\n",
    "                    #reverse search url for detailed information\n",
    "                    web_results = search(scraped_url, advanced=True, num_results=5)\n",
    "\n",
    "                    #check to see if link is already in the database\n",
    "                    for scraped_result in web_results:\n",
    "                        #make sure current link is not already in the database\n",
    "                        if not db.find_url(scraped_result.url):\n",
    "                            #track current result\n",
    "                            curr_data = {}\n",
    "                            #save resource url\n",
    "                            curr_data[\"resource\"] = scraped_result.url\n",
    "                            #save resource title\n",
    "                            curr_data[\"title\"] = scraped_result.title\n",
    "                            #save resource description\n",
    "                            curr_data[\"description\"] = scraped_result.description.replace(\"'\", \"\")\n",
    "                            #save resource topic\n",
    "                            curr_data[\"topic\"] = topic\n",
    "                            #save resource difficulty\n",
    "                            curr_data[\"difficulty\"] = difficulty\n",
    "                            #save resource validation\n",
    "                            curr_data[\"validated\"] = False\n",
    "                            #save resource found time\n",
    "                            curr_data[\"found_time\"] = datetime.now()\n",
    "                            #break out of current loop\n",
    "                            if curr_data:\n",
    "                                #append data to list\n",
    "                                results_data.append(curr_data)\n",
    "                                break\n",
    "                        else:\n",
    "                            #break out of loop since url is already in database\n",
    "                            print(f\"Link: {scraped_result.url} already exists in database\")\n",
    "                            break\n",
    "                    if len(results_data) >= 5:\n",
    "                        break\n",
    "                if len(results_data) >= 5:\n",
    "                    break\n",
    "\n",
    "#check to see if we have enough data to push to database\n",
    "if len(results_data) >= 5:\n",
    "    #loop through search results and insert\n",
    "    for data in results_data:\n",
    "        #insert current data into database\n",
    "        db.insert_resource(data)\n",
    "else:\n",
    "    print(\"Not enough data to commit\")\n",
    "\n",
    "#commit changes made in session\n",
    "db.commit_session()\n",
    "#end session\n",
    "db.end_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-integration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
